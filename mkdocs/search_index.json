{
    "docs": [
        {
            "location": "/", 
            "text": "Trumpet is an highly-available, fault-tolerant, non intrusive and scalable \n\nINotify\n-like building block for Hadoop \nHDFS\n.\n\n\n\n\nContext\n\n\nIn real world Hadoop deployments it's common to run several HDFS clusters. \nWhen doing so, one of the challenge is to replicate the data from one side to another while finding \nthe right trade-offs between the time before the data is replicated \nand impact on the existing infrastructure.\n\n\nThis project originated from work at \nVeriSign\n and presented at \nHadoop Summit Amsterdam in 2014\n \n(Hadoop Event Notification System, see \nhttp://hortonworks.com/blog/congrats-hadoop-summit-community-choice-winners/\n). \nIt in turn brought inspiration to the community to build the HDFS-INotify building block, \nknown as \nHDFS-6634\n and \nHDFS-7446\n.\n\n\nBut HDFS-INotify is \nintrusive\n to the NameNode by design, i.e. it's built as an RPC command hitting directly the NameNode. \nThis has a bunch of advantages like allowing to have no other dependencies as well as enable (in the future) further processing \ndirectly on the in-memory directory structure. But in real life busy production clusters, where the first \ngoal of an Hadoop \nSRE\ns is to protect the \nNameNode\n, \nrunning a more decoupled version of HDFS-INotify would probably be preferred (see \nNotes\n below)\n\n\nSo was born the project called Trumpet, in reference of the noise the elephant is producing when shouting.\n\n\nFeatures\n\n\nTrumpet put a strong focus on the following areas:\n\n\n\n\nScalable\n -- relies on \nKafka\n for events distribution,\n\n\nNon intrusive\n with low footprint -- runs alongside (and not in) the NameNode or the JournalNode,\n\n\nHighly available\n and \nfault tolerant\n -- several instances run a leader election in \nZookeeper\n and recover from previous state,\n\n\nRolling upgrade\n ready -- knows how to deal with HDFS rolling upgrade,\n\n\nSimple\n -- writes your own client application in seconds,\n\n\nOps friendly\n -- provides tools to ensure completeness and correctness of the broadcast,\n\n\nCompatibility\n -- runs (i.e. tested) on Hadoop 2.6, 2.7, HDP 2.2, 2.3 and CDH 5.2, 5.3, 5.4,\n\n\nNear realtime\n -- detects new transactions within milliseconds*.\n\n\n\n\nTo dig into Trumpet's features and how it works, please go to the \narchitecture\n page.\n\n\nQuickstart\n\n\nServer\n\n\n\n\nClone the project and build it\n  \nmvn clean install\n\n\nInstall the RPM generated in \nserver/target\n\n\nCreate a topic (Trumpet does not create the topic itself). For instance, for Kakfa 0.8.2-cp\n  \nkafka-topics --create --zookeeper \nzk_ip:2181\n --replication-factor 4 --partitions 1 --topic hdfs.inotify.events\n\n\nStart it.\n  \n/opt/trumpet/server/bin/trumpet.sh --zk.connect \nzk_ip:2181\n\n\n\n\nOr user the \nsupervisord\n \nconfig file\n provided.\n\n\nFor more details about the server side, please go to the \ninstallation\n page.\n\n\nClient Application\n\n\nAdd the dependency to the Trumpet client in your project\n\n\n        \ndependency\n\n            \ngroupId\ncom.verisign.vscc.trumpet\n/groupId\n\n            \nartifactId\ntrumpet-client\n/artifactId\n\n            \nversion\n${trumpet.version}\n/version\n\n        \n/dependency\n\n\n\n\n\nHint: the versions follow carefully the tags naming convention. Looks at the available tags in the project to get the version.\n\n\nNow it's as easy as using a Java \nIterable\n.\n\n\nString kafkaTopic = ...\nString zkConnect = ...\nfor (Map\nString, Object\n event : new TrumpetEventStreamer(curatorFramework, kafkaTopic)) {\n    ... do something with your event!\n}\n\n\n\n\nOne can also specify a start transaction id to start reading from a known point in time, as well as a stop transaction id\nto read only a subset of transactions.\n\n\nFor more details about the client side, please go to the \napplications\n page.\n\n\nContributing to Trumpet\n\n\nCode contributions, bug reports, feature requests etc. are all welcome.\n\n\nIf you are new to \nGitHub\n please read \nContributing to a project\n \nfor how to send patches and pull requests to Trumpet.\n\n\nAuthors\n\n\nThe initial authors of Trumpet are: \n\n\n\n\nBenoit Perroud\n\n\nHari Kuppuswamy\n\n\nPaula Morais\n\n\n\n\nAnd special thanks to \nJames Thomas\n \nand \nColin Patrick McCabe\n \nwho wrote the community version of HDFS INotify.\n\n\nNotes\n\n\nCloudera \nreverted\n \nHDFS-7929\n in \nCDH 5.4.2\n because of issues with INotify in rolling upgrades.", 
            "title": "What is Trumpet"
        }, 
        {
            "location": "/#context", 
            "text": "In real world Hadoop deployments it's common to run several HDFS clusters. \nWhen doing so, one of the challenge is to replicate the data from one side to another while finding \nthe right trade-offs between the time before the data is replicated \nand impact on the existing infrastructure.  This project originated from work at  VeriSign  and presented at  Hadoop Summit Amsterdam in 2014  \n(Hadoop Event Notification System, see  http://hortonworks.com/blog/congrats-hadoop-summit-community-choice-winners/ ). \nIt in turn brought inspiration to the community to build the HDFS-INotify building block, \nknown as  HDFS-6634  and  HDFS-7446 .  But HDFS-INotify is  intrusive  to the NameNode by design, i.e. it's built as an RPC command hitting directly the NameNode. \nThis has a bunch of advantages like allowing to have no other dependencies as well as enable (in the future) further processing \ndirectly on the in-memory directory structure. But in real life busy production clusters, where the first \ngoal of an Hadoop  SRE s is to protect the  NameNode , \nrunning a more decoupled version of HDFS-INotify would probably be preferred (see  Notes  below)  So was born the project called Trumpet, in reference of the noise the elephant is producing when shouting.", 
            "title": "Context"
        }, 
        {
            "location": "/#features", 
            "text": "Trumpet put a strong focus on the following areas:   Scalable  -- relies on  Kafka  for events distribution,  Non intrusive  with low footprint -- runs alongside (and not in) the NameNode or the JournalNode,  Highly available  and  fault tolerant  -- several instances run a leader election in  Zookeeper  and recover from previous state,  Rolling upgrade  ready -- knows how to deal with HDFS rolling upgrade,  Simple  -- writes your own client application in seconds,  Ops friendly  -- provides tools to ensure completeness and correctness of the broadcast,  Compatibility  -- runs (i.e. tested) on Hadoop 2.6, 2.7, HDP 2.2, 2.3 and CDH 5.2, 5.3, 5.4,  Near realtime  -- detects new transactions within milliseconds*.   To dig into Trumpet's features and how it works, please go to the  architecture  page.", 
            "title": "Features"
        }, 
        {
            "location": "/#quickstart", 
            "text": "", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#server", 
            "text": "Clone the project and build it\n   mvn clean install  Install the RPM generated in  server/target  Create a topic (Trumpet does not create the topic itself). For instance, for Kakfa 0.8.2-cp\n   kafka-topics --create --zookeeper  zk_ip:2181  --replication-factor 4 --partitions 1 --topic hdfs.inotify.events  Start it.\n   /opt/trumpet/server/bin/trumpet.sh --zk.connect  zk_ip:2181   Or user the  supervisord   config file  provided.  For more details about the server side, please go to the  installation  page.", 
            "title": "Server"
        }, 
        {
            "location": "/#client-application", 
            "text": "Add the dependency to the Trumpet client in your project           dependency \n             groupId com.verisign.vscc.trumpet /groupId \n             artifactId trumpet-client /artifactId \n             version ${trumpet.version} /version \n         /dependency   Hint: the versions follow carefully the tags naming convention. Looks at the available tags in the project to get the version.  Now it's as easy as using a Java  Iterable .  String kafkaTopic = ...\nString zkConnect = ...\nfor (Map String, Object  event : new TrumpetEventStreamer(curatorFramework, kafkaTopic)) {\n    ... do something with your event!\n}  One can also specify a start transaction id to start reading from a known point in time, as well as a stop transaction id\nto read only a subset of transactions.  For more details about the client side, please go to the  applications  page.", 
            "title": "Client Application"
        }, 
        {
            "location": "/#contributing-to-trumpet", 
            "text": "Code contributions, bug reports, feature requests etc. are all welcome.  If you are new to  GitHub  please read  Contributing to a project  \nfor how to send patches and pull requests to Trumpet.", 
            "title": "Contributing to Trumpet"
        }, 
        {
            "location": "/#authors", 
            "text": "The initial authors of Trumpet are:    Benoit Perroud  Hari Kuppuswamy  Paula Morais   And special thanks to  James Thomas  \nand  Colin Patrick McCabe  \nwho wrote the community version of HDFS INotify.  Notes  Cloudera  reverted   HDFS-7929  in  CDH 5.4.2  because of issues with INotify in rolling upgrades.", 
            "title": "Authors"
        }, 
        {
            "location": "/architecture/", 
            "text": "The strongest requirement behind this project is to be completely decouple from the NameNode for reading the transactions \nand streaming them to multiple clients.\n\n\nThe selected approach is thus to read the edits log file directly from the local filesystem from either the NameNode or, \nmore recommended, the JournalNode, and stream the interesting transactions into \nKafka\n. \nMany clients might in turn read from Kafka the complete set of transactions. The process of reading the edits log \nis totally independent from the NameNode or JournalNode and thus non-intrusive at all. \nAnd of course, the Kafka cluster can be scaled out independently.\n\n\nThe 10K feet architecture diagram would look like this:\n\n\n\n\nPoll the edit log directory\n\n\nTrumpet aims at reading from outside the NameNode / JournalNode process. The easiest solution for that is to rely \non the \nOffline Edits Viewer\n, \nor OEV to read the edits log files, just like \nRedo logs\n will be parsed in a \nRDBMS world. The edits log directory is the traversed to find the latest edit log file and read the transaction from there. \nThis edits log directory is either given by the configuration option \ndfs.journalnode.name.dir\n (\ndfs.namenode.name.dir\n respectively \nfor the NameNode) or set in as a command line argument of Trumpet at startup (see \ninstallation\n), and is later \nreferred as \ndfs.*.name.dir\n.\n\n\nBased on naming convention of the edits log dir (read more on \nhdfs metadata directory\n), \nit's straight forward to find the file containing a given transaction and resuming the read from this transaction. \nHadoop source code provides all the primitive functions to achieve that in few lines of code.\n\n\nPublish into Kafka\n\n\nKafka publisher-subscriber model is a good fit to push the transactions into and allow several clients to read from. \nIn current implementation supports \nKafka 0.8.2.1\n \nand \nKafka 0.8.1.1\n. \nWhile the choice is let to the user, it is recommended to use a topic with one single partition to guarantee \nconsistent ordering of the transactions. This recommendation might change in the future though. Several replica are \nhowever strongly recommended.\n\n\nTrumpet requires to have the topic created in advance. The default topic name is \nhdfs.inotify.events\n.\nAn example of topic creation would be:\n\n\n\n\nKakfa 0.8.1\n\n\n\n\n$ $KAFKA_HOME/bin/kafka-topic.sh --create --zookeeper \nzk_ip:2181\n --replication-factor 4 --partitions 1 --topic hdfs.inotify.events\n\n\n\n\n\n\nKakfa 0.8.2-cp\n\n\n\n\n$ kafka-topics --create --zookeeper \nzk_ip:2181\n --replication-factor 4 --partitions 1 --topic hdfs.inotify.events\n\n\n\n\nOf course the scalability of the client applications reading the transactions will be influenced by \nthe scalability of your Kafka cluster. Trumpet guarantees \nexactly once\n delivery of the HDFS event, \nbut does not guarantee any transaction or reader persistence to the client application. \nThis responsibility is delegated to the clients and most likely the Kafka consumer group.\n\n\nEvents as JSON Dictionary\n\n\nINotify interface changed between Hadoop 2.6 and Hadoop 2.7. In order to provide compatibility across these different versions \nevents are published in Kafka as simple JSON dictionary. \nIn the client Java API, the event is retrieved back as a simple \nMap\n.\nSee the section on \nwriting client applications\n for more details about the events format.\n\n\nLeader Election\n\n\nHadoop runs with HA in mind, and this project follows the same concept. Trumpet is designed to run alongside the JournalNode \n(or NameNode) process, reading from the local \ndfs.*.name.dir\n directory. The idea is to run one Trumpet server process per \nJournalNode, the processes running a leader election in Zookeeper using \nCurator\n \n\nrecipe\n to guarantee only one active process \nat the same time. The processes are also monitoring the JournalNode process, releasing quickly the leadership \nif the JournalNode process died.\n\n\nResume from previous run\n\n\nOnce a Trumpet server process become active (leader), it will find out from Kafka which was the latest published transaction, \nresuming the operation from there. In other words, Trumpet does guarantee exactly once delivery of the transactions, except\nin two well identified corner cases\n\n\n\n\nin case of prolonged downtime, Trumpet resumes with the oldest found transaction in \nthe local \ndfs.*.name.dir\n directory.\n\n\nwhen the server crashed in the middle of a \nConcatDeleteOp\n operation, which is a sequence of Append, Unlink* and Close\n all sharing the same transaction id, it's not straight forward to recover properly. Worst case, the operation might\n be truncated.\n\n\n\n\nRolling upgrade\n\n\nIn case of production usage of Trumpet, rolling upgrades need to be addressed. With its leader election \nfeature, Trumpet rolling upgrade is as easy as upgrading one Trumpet worker at the time. \nA tool which tells you who are the Trumpet workers and which one is active is also provided, \nsee the \noperations\n section.\n\n\nIn details, a rolling upgrade looks like: \n1. stop Trumpet, \n2. upgrade the JournalNode, \n3. restart the JournalNode, \n4. upgrade Trumpet, \n5. restart Trumpet\n\n\nRepeat for all the JournalNode/Trumpet instances.\n\n\nZookeeper separation\n\n\nZookeeper is a critical component of the Hadoop infrastructure, and it's common to split Zookeeper cluster, \none for the infrastructure components, like Kafka, NameNode etc... and another Zookeeper for more \nuser-space applications.\nIn Trumpet, you can either use one Zookeeper cluster, or split the Zookeeper usages between \nKafka discovery and leader election. Use:\n\n\n\n\n--zk.connect\n if you have one Zookeeper cluster\n\n\n--zk.connect.kafka\n and \n--zk.connect.user\n if you have two Zookeeper clusters.", 
            "title": "How it works"
        }, 
        {
            "location": "/architecture/#poll-the-edit-log-directory", 
            "text": "Trumpet aims at reading from outside the NameNode / JournalNode process. The easiest solution for that is to rely \non the  Offline Edits Viewer , \nor OEV to read the edits log files, just like  Redo logs  will be parsed in a \nRDBMS world. The edits log directory is the traversed to find the latest edit log file and read the transaction from there. \nThis edits log directory is either given by the configuration option  dfs.journalnode.name.dir  ( dfs.namenode.name.dir  respectively \nfor the NameNode) or set in as a command line argument of Trumpet at startup (see  installation ), and is later \nreferred as  dfs.*.name.dir .  Based on naming convention of the edits log dir (read more on  hdfs metadata directory ), \nit's straight forward to find the file containing a given transaction and resuming the read from this transaction. \nHadoop source code provides all the primitive functions to achieve that in few lines of code.", 
            "title": "Poll the edit log directory"
        }, 
        {
            "location": "/architecture/#publish-into-kafka", 
            "text": "Kafka publisher-subscriber model is a good fit to push the transactions into and allow several clients to read from. \nIn current implementation supports  Kafka 0.8.2.1  \nand  Kafka 0.8.1.1 . \nWhile the choice is let to the user, it is recommended to use a topic with one single partition to guarantee \nconsistent ordering of the transactions. This recommendation might change in the future though. Several replica are \nhowever strongly recommended.  Trumpet requires to have the topic created in advance. The default topic name is  hdfs.inotify.events .\nAn example of topic creation would be:   Kakfa 0.8.1   $ $KAFKA_HOME/bin/kafka-topic.sh --create --zookeeper  zk_ip:2181  --replication-factor 4 --partitions 1 --topic hdfs.inotify.events   Kakfa 0.8.2-cp   $ kafka-topics --create --zookeeper  zk_ip:2181  --replication-factor 4 --partitions 1 --topic hdfs.inotify.events  Of course the scalability of the client applications reading the transactions will be influenced by \nthe scalability of your Kafka cluster. Trumpet guarantees  exactly once  delivery of the HDFS event, \nbut does not guarantee any transaction or reader persistence to the client application. \nThis responsibility is delegated to the clients and most likely the Kafka consumer group.", 
            "title": "Publish into Kafka"
        }, 
        {
            "location": "/architecture/#events-as-json-dictionary", 
            "text": "INotify interface changed between Hadoop 2.6 and Hadoop 2.7. In order to provide compatibility across these different versions \nevents are published in Kafka as simple JSON dictionary. \nIn the client Java API, the event is retrieved back as a simple  Map .\nSee the section on  writing client applications  for more details about the events format.", 
            "title": "Events as JSON Dictionary"
        }, 
        {
            "location": "/architecture/#leader-election", 
            "text": "Hadoop runs with HA in mind, and this project follows the same concept. Trumpet is designed to run alongside the JournalNode \n(or NameNode) process, reading from the local  dfs.*.name.dir  directory. The idea is to run one Trumpet server process per \nJournalNode, the processes running a leader election in Zookeeper using  Curator   recipe  to guarantee only one active process \nat the same time. The processes are also monitoring the JournalNode process, releasing quickly the leadership \nif the JournalNode process died.", 
            "title": "Leader Election"
        }, 
        {
            "location": "/architecture/#resume-from-previous-run", 
            "text": "Once a Trumpet server process become active (leader), it will find out from Kafka which was the latest published transaction, \nresuming the operation from there. In other words, Trumpet does guarantee exactly once delivery of the transactions, except\nin two well identified corner cases   in case of prolonged downtime, Trumpet resumes with the oldest found transaction in \nthe local  dfs.*.name.dir  directory.  when the server crashed in the middle of a  ConcatDeleteOp  operation, which is a sequence of Append, Unlink* and Close\n all sharing the same transaction id, it's not straight forward to recover properly. Worst case, the operation might\n be truncated.", 
            "title": "Resume from previous run"
        }, 
        {
            "location": "/architecture/#rolling-upgrade", 
            "text": "In case of production usage of Trumpet, rolling upgrades need to be addressed. With its leader election \nfeature, Trumpet rolling upgrade is as easy as upgrading one Trumpet worker at the time. \nA tool which tells you who are the Trumpet workers and which one is active is also provided, \nsee the  operations  section.  In details, a rolling upgrade looks like: \n1. stop Trumpet, \n2. upgrade the JournalNode, \n3. restart the JournalNode, \n4. upgrade Trumpet, \n5. restart Trumpet  Repeat for all the JournalNode/Trumpet instances.", 
            "title": "Rolling upgrade"
        }, 
        {
            "location": "/architecture/#zookeeper-separation", 
            "text": "Zookeeper is a critical component of the Hadoop infrastructure, and it's common to split Zookeeper cluster, \none for the infrastructure components, like Kafka, NameNode etc... and another Zookeeper for more \nuser-space applications.\nIn Trumpet, you can either use one Zookeeper cluster, or split the Zookeeper usages between \nKafka discovery and leader election. Use:   --zk.connect  if you have one Zookeeper cluster  --zk.connect.kafka  and  --zk.connect.user  if you have two Zookeeper clusters.", 
            "title": "Zookeeper separation"
        }, 
        {
            "location": "/installation/", 
            "text": "Project structure\n\n\nThe current project is composed of 4 sub-projects:\n\n\n\n\n\n\ncommon\n\n  The \ncommon\n sub-project is where the reusable code is shared across the projects.\n\n\n\n\n\n\nserver\n\n  Trumpet server, i.e. reading the hdfs edits log files. Build this project to \ndeploy and run Trumpet\n\n\n\n\n\n\nclient\n\n  Example of client application, reading from Kakfa and doing someting meaningful. This sub-project also\n  contains the required dependencies to write \nclient applications\n.\n\n\n\n\n\n\ndocs\n\n  This documentation.\n\n\n\n\n\n\nBuild it\n\n\nThis project is based on \nMaven\n. To build it, you need to ensure \nyou have java and maven installed, and then: \n\n\nmvn clean install\n\n\n\n\nAvailable options, in addition to the standard maven options:\n\n\n\n\n-Phadoop26\n (default), \n-Phadoop27\n, \n-Pcdh52\n, \n-Pcdh53\n, \n-Pcdh54\n or \n-Phdp22\n. See \nCompatibility\n for details.\n\n\n-DrpmRelease=1\n to change the RPM build release number. See \nRPM\n for details.\n\n\n\n\nRPM\n\n\nRunning \nmvn package\n (included in the \ninstall\n goal) will generate an RPM. \nfind server/target -name \"*.rpm\"\n to list \nthe exact path and name of the server RPM.\n\n\nThe RPM can be transformed in .deb via Alien or FPM (see \nhttps://www.howtoforge.com/converting_rpm_to_deb_with_alien\n \nor \nhttps://github.com/jordansissel/fpm\n for more generic conversion) if required.\n\n\nThe RPM is installing Trumpet under \n/opt/trumpet/server/\n, with the following directory structure:\n\n\n/opt/trumpet/server/\n  +- bin/\n  |  +- events-validator.sh\n  |  +- peek-events.sh\n  |  +- status.sh\n  |  -- trumpet.sh\n  +- conf/\n  |  -- logback-production.xml\n  +- lib/\n  |  +- server-\nVERSION\n.jar\n  |  +- server.jar -\n server-\nVERSION\n.jar\n  |  -- ... all the other jars ...\n  -- logs/\n     +- trumpet.log\n     +- stdout\n     -- stderr\n\n\n\n\nWhile building Trumpet, you can add \n-DrpmRelease=123\n as a maven command parameter to change the RPM build number (by default \n1\n)\n\n\nNote: if you just want to build a specific version of the server RPM, without rebuilding everything, you need to run the following:\n\n\nmvn build-helper:parse-version rpm:attached-rpm -Phadoop27 -DrpmRelease=123 -f server\n\n\n\n\nCompatibility\n\n\nHDFS-7446\n, released as part of \n\nHadoop 2.7.0 release\n, \nchanged the HDFS-Inotify interface. In order to achieve multi-version compatibility, Trumpet is using a \n\ncompatibility class\n \nwhich uses reflection to work around compatibility issues.\n\n\nTrumpet is tested with:\n\n\n\n\nHadoop 2.6 -- \nmvn clean install -Phadoop26\n (default)\n\n\nHadoop 2.7 -- \nmvn clean install -Phadoop27\n\n\nCDH 5 -- \nmvn clean install -Pcdh52\n, \nmvn clean install -Pcdh53\n or \nmvn clean install -Pcdh54\n\n\nHDP 2.2, 2.3 -- \nmvn clean install -Phdp22\n, \nmvn clean install -Phdp23\n\n\n\n\nRun it\n\n\nTrumpet server is a long running process to run alongside the JournalNode. A \nsupervisord\n \n\nconfig file\n is also provided if you'd like to use supervisord daemon to launch Trumpet.\n\n\nOnce the RPM (or equivalent) is installed, manually launch Trumpet like this:\n\n\n$ /opt/trumpet/server/bin/trumpet.sh --zk.connect \nzk_ip:2181\n\n\n\n\n\nNote: If you're running CDH, it's highly likely that \ndfs.journalnode.name.dir\n is not in \n/etc/hadoop/conf/hdfs-site.xml\n. \nYou need to manually set \n--dfs.edits.dir\n parameters to specify manually the directory:\n\n\n$ /opt/trumpet/server/bin/trumpet.sh --zk.connect \nzk_ip:2181\n --dfs.edits.dir \nroot_dir_where_edits_files_are_stored\n\n\n\n\n\nNote #2: \nroot_dir_where_edits_files_are_stored\n should point to the same location than \ndfs.journalnode.name.dir\n, \ni.e. excluding \nnameservice\n/current\n.\n\n\nOr copy \n/opt/trumpet/server/conf/trumpet-server.ini\n in \n/etc/supervisord.d\n to have have Trumpet \nrun as a supervisord process (recommended).\n\n\nsudo cp /opt/trumpet/server/config/trumpet-server.ini /etc/supervisord.d/\nsudo vi /etc/supervisord.d/trumpet-server.ini # update \nzk_ip:2181\n and potentially add --dfs.edits.dir if not found in /etc/hadood/conf/hdfs-site.xml\nsudo supervisorctl reread\nsudo supervisorctl add trumpet # will start Trumpet as autostart is turned on\n\n\n\n\nNote #3: all the scripts can be run with \n--help\n to get additional info", 
            "title": "I want it!"
        }, 
        {
            "location": "/installation/#project-structure", 
            "text": "The current project is composed of 4 sub-projects:    common \n  The  common  sub-project is where the reusable code is shared across the projects.    server \n  Trumpet server, i.e. reading the hdfs edits log files. Build this project to  deploy and run Trumpet    client \n  Example of client application, reading from Kakfa and doing someting meaningful. This sub-project also\n  contains the required dependencies to write  client applications .    docs \n  This documentation.", 
            "title": "Project structure"
        }, 
        {
            "location": "/installation/#build-it", 
            "text": "This project is based on  Maven . To build it, you need to ensure \nyou have java and maven installed, and then:   mvn clean install  Available options, in addition to the standard maven options:   -Phadoop26  (default),  -Phadoop27 ,  -Pcdh52 ,  -Pcdh53 ,  -Pcdh54  or  -Phdp22 . See  Compatibility  for details.  -DrpmRelease=1  to change the RPM build release number. See  RPM  for details.", 
            "title": "Build it"
        }, 
        {
            "location": "/installation/#rpm", 
            "text": "Running  mvn package  (included in the  install  goal) will generate an RPM.  find server/target -name \"*.rpm\"  to list \nthe exact path and name of the server RPM.  The RPM can be transformed in .deb via Alien or FPM (see  https://www.howtoforge.com/converting_rpm_to_deb_with_alien  \nor  https://github.com/jordansissel/fpm  for more generic conversion) if required.  The RPM is installing Trumpet under  /opt/trumpet/server/ , with the following directory structure:  /opt/trumpet/server/\n  +- bin/\n  |  +- events-validator.sh\n  |  +- peek-events.sh\n  |  +- status.sh\n  |  -- trumpet.sh\n  +- conf/\n  |  -- logback-production.xml\n  +- lib/\n  |  +- server- VERSION .jar\n  |  +- server.jar -  server- VERSION .jar\n  |  -- ... all the other jars ...\n  -- logs/\n     +- trumpet.log\n     +- stdout\n     -- stderr  While building Trumpet, you can add  -DrpmRelease=123  as a maven command parameter to change the RPM build number (by default  1 )  Note: if you just want to build a specific version of the server RPM, without rebuilding everything, you need to run the following:  mvn build-helper:parse-version rpm:attached-rpm -Phadoop27 -DrpmRelease=123 -f server", 
            "title": "RPM"
        }, 
        {
            "location": "/installation/#compatibility", 
            "text": "HDFS-7446 , released as part of  Hadoop 2.7.0 release , \nchanged the HDFS-Inotify interface. In order to achieve multi-version compatibility, Trumpet is using a  compatibility class  \nwhich uses reflection to work around compatibility issues.  Trumpet is tested with:   Hadoop 2.6 --  mvn clean install -Phadoop26  (default)  Hadoop 2.7 --  mvn clean install -Phadoop27  CDH 5 --  mvn clean install -Pcdh52 ,  mvn clean install -Pcdh53  or  mvn clean install -Pcdh54  HDP 2.2, 2.3 --  mvn clean install -Phdp22 ,  mvn clean install -Phdp23", 
            "title": "Compatibility"
        }, 
        {
            "location": "/installation/#run-it", 
            "text": "Trumpet server is a long running process to run alongside the JournalNode. A  supervisord   config file  is also provided if you'd like to use supervisord daemon to launch Trumpet.  Once the RPM (or equivalent) is installed, manually launch Trumpet like this:  $ /opt/trumpet/server/bin/trumpet.sh --zk.connect  zk_ip:2181   Note: If you're running CDH, it's highly likely that  dfs.journalnode.name.dir  is not in  /etc/hadoop/conf/hdfs-site.xml . \nYou need to manually set  --dfs.edits.dir  parameters to specify manually the directory:  $ /opt/trumpet/server/bin/trumpet.sh --zk.connect  zk_ip:2181  --dfs.edits.dir  root_dir_where_edits_files_are_stored   Note #2:  root_dir_where_edits_files_are_stored  should point to the same location than  dfs.journalnode.name.dir , \ni.e. excluding  nameservice /current .  Or copy  /opt/trumpet/server/conf/trumpet-server.ini  in  /etc/supervisord.d  to have have Trumpet \nrun as a supervisord process (recommended).  sudo cp /opt/trumpet/server/config/trumpet-server.ini /etc/supervisord.d/\nsudo vi /etc/supervisord.d/trumpet-server.ini # update  zk_ip:2181  and potentially add --dfs.edits.dir if not found in /etc/hadood/conf/hdfs-site.xml\nsudo supervisorctl reread\nsudo supervisorctl add trumpet # will start Trumpet as autostart is turned on  Note #3: all the scripts can be run with  --help  to get additional info", 
            "title": "Run it"
        }, 
        {
            "location": "/applications/", 
            "text": "Trumpet comes out as a new building block in the Hadoop ecosystem \nand unleashes brand new capabilities to HDFS. As it relies mostly on Kafka \nfor the distribution of the events to the clients, it is decoupled from the NameNode\nand does not impact its operations. It also scales as well as your Kafka cluster.\n\n\nBelow are the steps to let you simply and quickly write your own client applications:\n\n\nDependency\n\n\nAdd the dependency to the Trumpet client in your project\n\n\n        \ndependency\n\n            \ngroupId\ncom.verisign.vscc.trumpet\n/groupId\n\n            \nartifactId\ntrumpet-client\n/artifactId\n\n            \nversion\n${trumpet.version}\n/version\n\n        \n/dependency\n\n\n\n\n\nHint: the versions follow carefully the tags naming convention. Looks at the available tags in the project to get the version.\n\n\nUse it\n\n\nNow it's as easy as using a Java \nIterable\n.\n\n\nString kafkaTopic = ...\nString zkConnect = ...\nfor (Map\nString, Object\n event : new TrumpetEventStreamer(curatorFramework, kafkaTopic)) {\n    ... do something with your event!\n}\n\n\n\n\nDelivery guarantee\n\n\nAs Trumpet relies on Kafka, the delivery guarantee of Kafka is inherited for the client side.\n\n\nTrumpetEventStreamer\n uses internally a \nsimple consumer\n\nand does not store any offset anywhere, the checkpointing process is offloaded to the application writer. \nBut a \nKafka consumer group\n can be used instead,\nand configured as required to fit you application needs.\n\n\nTest it\n\n\nA test environment is available in the \ntrumpet-client:tests\n jar. To import it inside your maven project, \nuse the following dependency:\n\n\n        \ndependency\n\n            \ngroupId\ncom.verisign.vscc.trumpet\n/groupId\n\n            \nartifactId\ntrumpet-client\n/artifactId\n\n            \nversion\n${trumpet.version}\n/version\n\n            \nclassifier\ntests\n/classifier\n\n            \nscope\ntest\n/scope\n\n        \n/dependency\n\n\n\n\n\nYou can then write test classes extending \ncom.verisign.vscc.hdfs.trumpet.server.IntegrationTest\n and you'll have a \ncomplete running environment, including Zookeeper, Kakfa and \n\nmini-hdfs\n.\n\n\nEvents Format\n\n\nThe events published into Kakfa are JSON dictionary. The rationale behind that is discussed \nhere\n.\n\n\nEvents taken into account are basically all events modifying the NameNode \n\nINodeDirectory\n \nin-memory structure. The exhaustive list is given below, and they are regrouped in 6 event types: \n\nCREATE\n, \nCLOSE\n, \nAPPEND\n, \nRENAME\n, \nMETADATA\n, \nUNLINK\n:\n\n\n\n\nAdd -- either \nCREATE\n or \nAPPEND\n\n\nClose -- \nCLOSE\n\n\nSet Replication -- \nMETADATA\n\n\nConcatDelete -- create a sequence of \nAPPEND\n, \nUNLINK\n and \nCLOSE\n, but all with the same \ntxId\n (see \nnotes\n)\n\n\nRenameOld -- \nRENAME\n\n\nRename -- \nRENAME\n\n\nDelete -- \nUNLINK\n\n\nMkdir -- \nCREATE\n\n\nSetPermissions -- \nMETADATA\n\n\nSetOwner -- \nMETADATA\n\n\nTimes -- \nMETADATA\n\n\nSymlink -- \nCREATE\n\n\nRemoveXAttr -- \nMETADATA\n\n\nSetXAttr -- \nMETADATA\n\n\nSetAcl -- \nMETADATA\n\n\n\n\nFind more details about the operation decoding and translation in the class \n\norg.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator\n \nand \n\norg.apache.hadoop.hdfs.inotify.Event\n.\n\n\nThe available attributes in the JSON dict per event types are described below.\n\n\n\n\nAPPEND\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nAPPEND\n\n\ntype of event.\n\n\n\n\n\n\npath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCLOSE\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nCLOSE\n\n\ntype of event.\n\n\n\n\n\n\npath\n\n\n\n\n\n\n\n\n\n\nfileSize\n\n\n\n\n\n\n\n\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCREATE\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nCREATE\n\n\ntype of event.\n\n\n\n\n\n\niNodeType\n\n\n\n\nValue can be FILE, DIRECTORY or SYMLINK. See org.apache.hadoop.hdfs.inotify.Event.INodeType for more details.\n\n\n\n\n\n\npath\n\n\n\n\n\n\n\n\n\n\nctime\n\n\n\n\n\n\n\n\n\n\nreplication\n\n\n\n\n\n\n\n\n\n\nownerName\n\n\n\n\n\n\n\n\n\n\ngroupName\n\n\n\n\n\n\n\n\n\n\nperms\n\n\n\n\n\n\n\n\n\n\nsymlinkTarget\n\n\n\n\n\n\n\n\n\n\noverwrite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMETADATA\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nMETADATA\n\n\ntype of event.\n\n\n\n\n\n\npath\n\n\n\n\n\n\n\n\n\n\nmetadataType\n\n\n\n\n\n\n\n\n\n\nmtime\n\n\n\n\n\n\n\n\n\n\natime\n\n\n\n\n\n\n\n\n\n\nreplication\n\n\n\n\n\n\n\n\n\n\nownerName\n\n\n\n\n\n\n\n\n\n\ngroupName\n\n\n\n\n\n\n\n\n\n\nperms\n\n\n\n\n\n\n\n\n\n\nacls\n\n\n\n\n\n\n\n\n\n\nxAttrs\n\n\n\n\n\n\n\n\n\n\nxAttrsRemoved\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRENAME\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nRENAME\n\n\ntype of event.\n\n\n\n\n\n\nsrcPath\n\n\n\n\n\n\n\n\n\n\ndstPath\n\n\n\n\n\n\n\n\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUNLINK\n\n\n\n\n\n\n\n\n\n\nAttribute name\n\n\nMandatory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntxId\n\n\nYes\n\n\ntransaction Id\n\n\n\n\n\n\neventType\n\n\nUNLINK\n\n\ntype of event.\n\n\n\n\n\n\npath\n\n\n\n\n\n\n\n\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\nExample Application\n\n\ncom.verisign.vscc.hdfs.trumpet.client.example.TestApp\n \nis a sample application, listening to Trumpet and filtering out \n_SUCCESS\n files, indicating the success of a MapReduce application.\n\n\nIt would be really easy to start a distcp job from here to, for instance, replicate the newly created data into a different remote cluster.", 
            "title": "Write your Apps"
        }, 
        {
            "location": "/applications/#dependency", 
            "text": "Add the dependency to the Trumpet client in your project           dependency \n             groupId com.verisign.vscc.trumpet /groupId \n             artifactId trumpet-client /artifactId \n             version ${trumpet.version} /version \n         /dependency   Hint: the versions follow carefully the tags naming convention. Looks at the available tags in the project to get the version.", 
            "title": "Dependency"
        }, 
        {
            "location": "/applications/#use-it", 
            "text": "Now it's as easy as using a Java  Iterable .  String kafkaTopic = ...\nString zkConnect = ...\nfor (Map String, Object  event : new TrumpetEventStreamer(curatorFramework, kafkaTopic)) {\n    ... do something with your event!\n}  Delivery guarantee  As Trumpet relies on Kafka, the delivery guarantee of Kafka is inherited for the client side.  TrumpetEventStreamer  uses internally a  simple consumer \nand does not store any offset anywhere, the checkpointing process is offloaded to the application writer. \nBut a  Kafka consumer group  can be used instead,\nand configured as required to fit you application needs.", 
            "title": "Use it"
        }, 
        {
            "location": "/applications/#test-it", 
            "text": "A test environment is available in the  trumpet-client:tests  jar. To import it inside your maven project, \nuse the following dependency:           dependency \n             groupId com.verisign.vscc.trumpet /groupId \n             artifactId trumpet-client /artifactId \n             version ${trumpet.version} /version \n             classifier tests /classifier \n             scope test /scope \n         /dependency   You can then write test classes extending  com.verisign.vscc.hdfs.trumpet.server.IntegrationTest  and you'll have a \ncomplete running environment, including Zookeeper, Kakfa and  mini-hdfs .", 
            "title": "Test it"
        }, 
        {
            "location": "/applications/#events-format", 
            "text": "The events published into Kakfa are JSON dictionary. The rationale behind that is discussed  here .  Events taken into account are basically all events modifying the NameNode  INodeDirectory  \nin-memory structure. The exhaustive list is given below, and they are regrouped in 6 event types:  CREATE ,  CLOSE ,  APPEND ,  RENAME ,  METADATA ,  UNLINK :   Add -- either  CREATE  or  APPEND  Close --  CLOSE  Set Replication --  METADATA  ConcatDelete -- create a sequence of  APPEND ,  UNLINK  and  CLOSE , but all with the same  txId  (see  notes )  RenameOld --  RENAME  Rename --  RENAME  Delete --  UNLINK  Mkdir --  CREATE  SetPermissions --  METADATA  SetOwner --  METADATA  Times --  METADATA  Symlink --  CREATE  RemoveXAttr --  METADATA  SetXAttr --  METADATA  SetAcl --  METADATA   Find more details about the operation decoding and translation in the class  org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator  \nand  org.apache.hadoop.hdfs.inotify.Event .  The available attributes in the JSON dict per event types are described below.   APPEND      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  APPEND  type of event.    path        CLOSE      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  CLOSE  type of event.    path      fileSize      timestamp        CREATE      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  CREATE  type of event.    iNodeType   Value can be FILE, DIRECTORY or SYMLINK. See org.apache.hadoop.hdfs.inotify.Event.INodeType for more details.    path      ctime      replication      ownerName      groupName      perms      symlinkTarget      overwrite        METADATA      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  METADATA  type of event.    path      metadataType      mtime      atime      replication      ownerName      groupName      perms      acls      xAttrs      xAttrsRemoved        RENAME      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  RENAME  type of event.    srcPath      dstPath      timestamp        UNLINK      Attribute name  Mandatory  Description      txId  Yes  transaction Id    eventType  UNLINK  type of event.    path      timestamp", 
            "title": "Events Format"
        }, 
        {
            "location": "/applications/#example-application", 
            "text": "com.verisign.vscc.hdfs.trumpet.client.example.TestApp  \nis a sample application, listening to Trumpet and filtering out  _SUCCESS  files, indicating the success of a MapReduce application.  It would be really easy to start a distcp job from here to, for instance, replicate the newly created data into a different remote cluster.", 
            "title": "Example Application"
        }, 
        {
            "location": "/operations/", 
            "text": "To operate Trumpet on a day-to-day basis, tools are provided to help to understand quickly what's going on in Trumpet. \nAnd please keep in mind that all the tools are \n--help\n friendly, so abuse of the help!\n\n\nValidate correctness and completeness\n\n\nA tool is provided which goes back in time as much it can both in the kafka topic and \ndfs.*.name.dir\n \nand validate that all the transactions in the edit log dir are in Kafka and match:\n\n\n$ /opt/trumpet/server/bin/events-validator.sh --zk.connect \nzk_ip:2181\n --dfs.edits.dir \nroot_dir_where_edits_files_are_stored\n --numevents 1000000\n\n\n\n\nPeek Kafka topic\n\n\nAt any time, you can peek the Kafka topic, i.e. retrieve and display in a console the \nn\n latest messages:\n\n\n$ /opt/trumpet/server/bin/peek-events.sh --zk.connect \nzk_ip:2181\n --numevents 100\n\n\n\n\nIf \ncom.verisign.vscc.hdfs.trumpet.server.tool.PeekInotifyEvents\n logger is in debug mode, complete event is dumped into the command line. \nReally useful for debugging, while this might be noisy.\n\n\nWorkers status\n\n\nAn utility is also provided to display the running Trumpet workers as well as which one is the leader\n\n\n$ /opt/trumpet/server/bin/status.sh --zk.connect \nzk_ip:2181\n\n\n\n\n\nKafka Tools\n\n\nYou can also use \nplain Kafka tools\n to find out \nif there are events flowing into the topic.\n\n\nGetting the latest offset\n\n\nThis tool will show you the latest (i.e, most recent) offest of the topic. Calling this function several time will let you show \nhow many offset (i.e, inotify events) arrived in the topic:\n\n\nkafka-run-class kafka.tools.GetOffsetShell --broker-list \nbroker:port\n --time -1 --topic hdfs.inotify.events\n\n\n\n\nPeeking Kafka topic\n\n\nJust like the \npeek-events.sh\n Trumpet tool, you can display the messages that arrives in the topic because they are plain JSON messages:\n\n\nkafka-simple-consumer-shell --broker-list \nbroker:port\n --topic hdfs.inotify.events --offset -1 --max-messages 5 --partition 0\n\n\n\n\nGraphite Integration\n\n\nTrumpet relies on the awesome \nMetrics library\n to collect metrics, \nlike the transactionId actually processed, the number of event per seconds\npublished into Kafka, since when the current daemon is leader, ... \nSee the \nMetrics\n \nclass for more details.\n\n\nTo reports these metrics in \nGraphite\n (or compatible graphite collectors), simply \nrun \ntrumpet.sh\n with \n--graphite.server.hostport \nhost\n:\nport\n and you're all set. \nport\n is optional and is defaulted \nto 2003. \n--graphite.prefix\n can change the prefix of the metric reported in graphite, by default \ntrumpet.\nhostname\n.\n\n\nGraphite samples\n\n\n\n\n\n\nTypical Failure Illustrated\n\n\n\n\nTrumpet Server #1:\n\n\n\n\n2015-06-04 03:11:53.565 DEBUG [Curator-LeaderSelector-0] c.v.v.h.t.s.rx.EditLogObservable - Processed 104 tx and streamed 80 events to the observer. Started at 2154122762 to last decoded txId 2154122865\n...\n2015-06-04 03:11:54.280 WARN [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Got exception. Releasing leadership.\n\n\n\n\nBAM, died, for whatever reason. Last transaction published is 2154122865\n\n\n\n\nTrumpet Server #2:\n\n\n\n\nFew milliseconds later, this Trumpet server intance takes over, starting from 2154122866\n\n\n2015-06-04 03:11:54.530 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Elected as leader, let's stream!\n2015-06-04 03:11:54.595 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Retrieved lastPublishedTxId: 2154122865\n2015-06-04 03:11:54.661 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Reading editLog file /.../dfs/jn/journalhdfs1/current/edits_0000000002154116241-0000000002154123044 from tx 2154122866\n\n\n\n\nRunning the events-validator, as HDFS, to ensure that not transactions have been lost in between\n\n\n$ /opt/trumpet/server/bin/events-validator.sh --zk.connect ${zkConnect} --numevents 200000 --dfs.edits.dir /.../dfs/jn\nProcessing /app2/dfs/jn/journalhdfs1/current/edits_0000000002154094680-0000000002154105842 from tx 2154105374\nProcessing /app2/dfs/jn/journalhdfs1/current/edits_0000000002154105843-0000000002154116240 from tx 2154105843\n...\nGood, kafka topic hdfs.inotify.events and edit.log.dir com.verisign.vscc.hdfs.trumpet.server.editlog.EditLogDir[/.../dfs/jn/journalhdfs1/current] look consistent.\n\n\n\n\n\nAlright, no transaction lost!", 
            "title": "For DevOps"
        }, 
        {
            "location": "/operations/#validate-correctness-and-completeness", 
            "text": "A tool is provided which goes back in time as much it can both in the kafka topic and  dfs.*.name.dir  \nand validate that all the transactions in the edit log dir are in Kafka and match:  $ /opt/trumpet/server/bin/events-validator.sh --zk.connect  zk_ip:2181  --dfs.edits.dir  root_dir_where_edits_files_are_stored  --numevents 1000000", 
            "title": "Validate correctness and completeness"
        }, 
        {
            "location": "/operations/#peek-kafka-topic", 
            "text": "At any time, you can peek the Kafka topic, i.e. retrieve and display in a console the  n  latest messages:  $ /opt/trumpet/server/bin/peek-events.sh --zk.connect  zk_ip:2181  --numevents 100  If  com.verisign.vscc.hdfs.trumpet.server.tool.PeekInotifyEvents  logger is in debug mode, complete event is dumped into the command line. \nReally useful for debugging, while this might be noisy.", 
            "title": "Peek Kafka topic"
        }, 
        {
            "location": "/operations/#workers-status", 
            "text": "An utility is also provided to display the running Trumpet workers as well as which one is the leader  $ /opt/trumpet/server/bin/status.sh --zk.connect  zk_ip:2181", 
            "title": "Workers status"
        }, 
        {
            "location": "/operations/#kafka-tools", 
            "text": "You can also use  plain Kafka tools  to find out \nif there are events flowing into the topic.  Getting the latest offset  This tool will show you the latest (i.e, most recent) offest of the topic. Calling this function several time will let you show \nhow many offset (i.e, inotify events) arrived in the topic:  kafka-run-class kafka.tools.GetOffsetShell --broker-list  broker:port  --time -1 --topic hdfs.inotify.events  Peeking Kafka topic  Just like the  peek-events.sh  Trumpet tool, you can display the messages that arrives in the topic because they are plain JSON messages:  kafka-simple-consumer-shell --broker-list  broker:port  --topic hdfs.inotify.events --offset -1 --max-messages 5 --partition 0", 
            "title": "Kafka Tools"
        }, 
        {
            "location": "/operations/#graphite-integration", 
            "text": "Trumpet relies on the awesome  Metrics library  to collect metrics, \nlike the transactionId actually processed, the number of event per seconds\npublished into Kafka, since when the current daemon is leader, ... \nSee the  Metrics  \nclass for more details.  To reports these metrics in  Graphite  (or compatible graphite collectors), simply \nrun  trumpet.sh  with  --graphite.server.hostport  host : port  and you're all set.  port  is optional and is defaulted \nto 2003.  --graphite.prefix  can change the prefix of the metric reported in graphite, by default  trumpet. hostname .  Graphite samples", 
            "title": "Graphite Integration"
        }, 
        {
            "location": "/operations/#typical-failure-illustrated", 
            "text": "Trumpet Server #1:   2015-06-04 03:11:53.565 DEBUG [Curator-LeaderSelector-0] c.v.v.h.t.s.rx.EditLogObservable - Processed 104 tx and streamed 80 events to the observer. Started at 2154122762 to last decoded txId 2154122865\n...\n2015-06-04 03:11:54.280 WARN [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Got exception. Releasing leadership.  BAM, died, for whatever reason. Last transaction published is 2154122865   Trumpet Server #2:   Few milliseconds later, this Trumpet server intance takes over, starting from 2154122866  2015-06-04 03:11:54.530 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Elected as leader, let's stream!\n2015-06-04 03:11:54.595 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Retrieved lastPublishedTxId: 2154122865\n2015-06-04 03:11:54.661 DEBUG [Curator-LeaderSelector-0] c.v.v.h.trumpet.server.TrumpetLeader - Reading editLog file /.../dfs/jn/journalhdfs1/current/edits_0000000002154116241-0000000002154123044 from tx 2154122866  Running the events-validator, as HDFS, to ensure that not transactions have been lost in between  $ /opt/trumpet/server/bin/events-validator.sh --zk.connect ${zkConnect} --numevents 200000 --dfs.edits.dir /.../dfs/jn\nProcessing /app2/dfs/jn/journalhdfs1/current/edits_0000000002154094680-0000000002154105842 from tx 2154105374\nProcessing /app2/dfs/jn/journalhdfs1/current/edits_0000000002154105843-0000000002154116240 from tx 2154105843\n...\nGood, kafka topic hdfs.inotify.events and edit.log.dir com.verisign.vscc.hdfs.trumpet.server.editlog.EditLogDir[/.../dfs/jn/journalhdfs1/current] look consistent.  Alright, no transaction lost!", 
            "title": "Typical Failure Illustrated"
        }, 
        {
            "location": "/changelog/", 
            "text": "Trumpet Changelog.\n\n\nVersion 2.3.1\n\n\n\n\nAvoid message leak in the Infinite Streamer\n\n\nMake server more integration test friendly (contrib by vgrivel)\n\n\n\n\nVersion 2.3\n\n\n\n\n#PR1\n Move examples in a dedicated subproject\n\n\nAdd Trie data structure for efficient path filtering\n\n\nRemove dependency on github.com/jstanier/kafka-broker-discovery\n\n\nAdd HDP 2.3.4 support\n\n\nClean up dependencies -- importing trumpet-client in downstream project is safe\n\n\nSplit TrumpetEventStreamer in infinite (relying on Kafka Group Consumer) and Bounded (based on Kafka Simple Consumer)\n\n\n\n\nVersion 2.2\n\n\n\n\nInitial Public Release", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#version-231", 
            "text": "Avoid message leak in the Infinite Streamer  Make server more integration test friendly (contrib by vgrivel)", 
            "title": "Version 2.3.1"
        }, 
        {
            "location": "/changelog/#version-23", 
            "text": "#PR1  Move examples in a dedicated subproject  Add Trie data structure for efficient path filtering  Remove dependency on github.com/jstanier/kafka-broker-discovery  Add HDP 2.3.4 support  Clean up dependencies -- importing trumpet-client in downstream project is safe  Split TrumpetEventStreamer in infinite (relying on Kafka Group Consumer) and Bounded (based on Kafka Simple Consumer)", 
            "title": "Version 2.3"
        }, 
        {
            "location": "/changelog/#version-22", 
            "text": "Initial Public Release", 
            "title": "Version 2.2"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright \u00a9 2014-2015 \nVeriSign, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n\n\n                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.", 
            "title": "License"
        }
    ]
}